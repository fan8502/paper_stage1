{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import TSF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('Chapter2_after.txt', 'r', encoding = 'UTF-8') as inFile:\n",
    "    seg = inFile.readlines()\n",
    "seg = [s.replace('\\n', '').strip() for s in seg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Explain why achieving strategic fit is critical to a company's overall success.\",\n",
       " 'Describe how a company achieves strategic fit between its supply chain strategy and its competitive strategy.',\n",
       " 'Discuss the importance of expanding the scope of strategic fit across the supply chain.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seg[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg = 0.3418900908976691\n",
      "Dev = 0.25939590042438526\n"
     ]
    }
   ],
   "source": [
    "# TSF參數設定之參考論文\n",
    "# Improving Text Segmentation with Clustering Cohesion\n",
    "# Efficient linear text segmentation based on information retrieval techniques\n",
    "\n",
    "K = 8 # the number of sentences in a block\n",
    "THRESHOLD_DISSIM = 0.7 #根據Kern and Granitzer (2009)的建議訂為0.7。\n",
    "ParaSenList = TSF2.SegementPara(seg, K, THRESHOLD_DISSIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "60\n",
      "36\n",
      "1\n",
      "6\n",
      "9\n",
      "2\n",
      "1\n",
      "2\n",
      "59\n",
      "13\n",
      "1\n",
      "23\n",
      "12\n",
      "19\n",
      "1\n",
      "1\n",
      "32\n",
      "14\n",
      "19\n",
      "15\n",
      "8\n",
      "63\n",
      "1\n",
      "1\n",
      "11\n",
      "1\n",
      "46\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(ParaSenList)):\n",
    "    print (len(ParaSenList[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 讀取summarizer和word2vec模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/chenyifan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/chenyifan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import text_summarizer\n",
    "import nltk\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-01 21:38:51,883 : INFO : loading Word2Vec object from allWords_word2vec\n",
      "2019-10-01 21:38:52,139 : INFO : loading wv recursively from allWords_word2vec.wv.* with mmap=None\n",
      "2019-10-01 21:38:52,140 : INFO : setting ignored attribute vectors_norm to None\n",
      "2019-10-01 21:38:52,140 : INFO : loading vocabulary recursively from allWords_word2vec.vocabulary.* with mmap=None\n",
      "2019-10-01 21:38:52,141 : INFO : loading trainables recursively from allWords_word2vec.trainables.* with mmap=None\n",
      "2019-10-01 21:38:52,142 : INFO : setting ignored attribute cum_table to None\n",
      "2019-10-01 21:38:52,143 : INFO : loaded allWords_word2vec\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from gensim.models import word2vec\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "embedding_model = word2vec.Word2Vec.load(\"allWords_word2vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-01 21:53:40,108 : INFO : 'pattern' package not found; tag filters are not available for English\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import summa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 計算摘要（一次計算並寫檔）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-01 21:55:53,538 : WARNING : Input text is expected to have at least 10 sentences.\n",
      "2019-10-01 21:55:53,538 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-10-01 21:55:53,539 : INFO : built Dictionary(40 unique tokens: ['achiev', 'compani', 'critic', 'explain', 'fit']...) from 8 documents (total 85 corpus positions)\n",
      "2019-10-01 21:55:53,541 : WARNING : Input corpus is expected to have at least 10 documents.\n",
      "2019-10-01 21:55:53,542 : INFO : Building graph\n",
      "2019-10-01 21:55:53,542 : INFO : Filling graph\n",
      "2019-10-01 21:55:53,544 : INFO : Removing unreachable nodes of graph\n",
      "2019-10-01 21:55:53,545 : INFO : Pagerank graph\n",
      "2019-10-01 21:55:53,549 : INFO : Sorting pagerank scores\n",
      "2019-10-01 21:55:53,645 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-10-01 21:55:53,647 : INFO : built Dictionary(245 unique tokens: ['aim', 'avail', 'exampl', 'high', 'low']...) from 59 documents (total 646 corpus positions)\n",
      "2019-10-01 21:55:53,648 : INFO : Building graph\n",
      "2019-10-01 21:55:53,649 : INFO : Filling graph\n",
      "2019-10-01 21:55:53,668 : INFO : Removing unreachable nodes of graph\n",
      "2019-10-01 21:55:53,669 : INFO : Pagerank graph\n",
      "2019-10-01 21:55:53,684 : INFO : Sorting pagerank scores\n",
      "2019-10-01 21:55:53,785 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-10-01 21:55:53,787 : INFO : built Dictionary(178 unique tokens: ['abil', 'compani', 'consid', 'cost', 'distribut']...) from 36 documents (total 408 corpus positions)\n",
      "2019-10-01 21:55:53,788 : INFO : Building graph\n",
      "2019-10-01 21:55:53,791 : INFO : Filling graph\n",
      "2019-10-01 21:55:53,799 : INFO : Removing unreachable nodes of graph\n",
      "2019-10-01 21:55:53,800 : INFO : Pagerank graph\n",
      "2019-10-01 21:55:53,810 : INFO : Sorting pagerank scores\n",
      "2019-10-01 21:55:53,867 : WARNING : Input text is expected to have at least 10 sentences.\n",
      "2019-10-01 21:55:53,868 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-10-01 21:55:53,871 : INFO : built Dictionary(34 unique tokens: ['conveni', 'custom', 'deterg', 'look', 'lowest']...) from 6 documents (total 53 corpus positions)\n",
      "2019-10-01 21:55:53,873 : WARNING : Input corpus is expected to have at least 10 documents.\n",
      "2019-10-01 21:55:53,876 : INFO : Building graph\n",
      "2019-10-01 21:55:53,877 : INFO : Filling graph\n",
      "2019-10-01 21:55:53,884 : INFO : Removing unreachable nodes of graph\n",
      "2019-10-01 21:55:53,886 : INFO : Pagerank graph\n",
      "2019-10-01 21:55:53,895 : INFO : Sorting pagerank scores\n",
      "2019-10-01 21:55:53,918 : WARNING : Input text is expected to have at least 10 sentences.\n",
      "2019-10-01 21:55:53,919 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-10-01 21:55:53,923 : INFO : built Dictionary(48 unique tokens: ['attribut', 'custom', 'demand', 'differ', 'emerg']...) from 9 documents (total 104 corpus positions)\n",
      "2019-10-01 21:55:53,926 : WARNING : Input corpus is expected to have at least 10 documents.\n",
      "2019-10-01 21:55:53,931 : INFO : Building graph\n",
      "2019-10-01 21:55:53,934 : INFO : Filling graph\n",
      "2019-10-01 21:55:53,937 : INFO : Removing unreachable nodes of graph\n",
      "2019-10-01 21:55:53,938 : INFO : Pagerank graph\n",
      "2019-10-01 21:55:53,941 : INFO : Sorting pagerank scores\n",
      "2019-10-01 21:55:53,959 : WARNING : Input text is expected to have at least 10 sentences.\n",
      "2019-10-01 21:55:53,960 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-10-01 21:55:53,961 : INFO : built Dictionary(16 unique tokens: ['apparel', 'custom', 'depart', 'design', 'desir']...) from 2 documents (total 22 corpus positions)\n",
      "2019-10-01 21:55:53,963 : WARNING : Input corpus is expected to have at least 10 documents.\n",
      "2019-10-01 21:55:53,967 : INFO : Building graph\n",
      "2019-10-01 21:55:53,968 : INFO : Filling graph\n",
      "2019-10-01 21:55:53,971 : INFO : Removing unreachable nodes of graph\n",
      "2019-10-01 21:55:53,973 : WARNING : Please add more sentences to the text. The number of reachable nodes is below 3\n",
      "2019-10-01 21:55:53,975 : WARNING : Couldn't get relevant sentences.\n",
      "2019-10-01 21:55:54,004 : WARNING : Input text is expected to have at least 10 sentences.\n",
      "2019-10-01 21:55:54,005 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-10-01 21:55:54,008 : INFO : built Dictionary(16 unique tokens: ['attribut', 'combin', 'custom', 'demand', 'describ']...) from 2 documents (total 18 corpus positions)\n",
      "2019-10-01 21:55:54,010 : WARNING : Input corpus is expected to have at least 10 documents.\n",
      "2019-10-01 21:55:54,012 : INFO : Building graph\n",
      "2019-10-01 21:55:54,015 : INFO : Filling graph\n",
      "2019-10-01 21:55:54,017 : INFO : Removing unreachable nodes of graph\n",
      "2019-10-01 21:55:54,021 : WARNING : Please add more sentences to the text. The number of reachable nodes is below 3\n",
      "2019-10-01 21:55:54,027 : WARNING : Couldn't get relevant sentences.\n",
      "2019-10-01 21:55:54,134 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-10-01 21:55:54,136 : INFO : built Dictionary(222 unique tokens: ['demand', 'impli', 'uncertainti', 'appear', 'categori']...) from 59 documents (total 731 corpus positions)\n",
      "2019-10-01 21:55:54,137 : INFO : Building graph\n",
      "2019-10-01 21:55:54,138 : INFO : Filling graph\n",
      "2019-10-01 21:55:54,158 : INFO : Removing unreachable nodes of graph\n",
      "2019-10-01 21:55:54,159 : INFO : Pagerank graph\n",
      "2019-10-01 21:55:54,170 : INFO : Sorting pagerank scores\n",
      "2019-10-01 21:55:54,245 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-10-01 21:55:54,246 : INFO : built Dictionary(61 unique tokens: ['come', 'cost', 'respons', 'capac', 'demand']...) from 13 documents (total 130 corpus positions)\n",
      "2019-10-01 21:55:54,247 : INFO : Building graph\n",
      "2019-10-01 21:55:54,250 : INFO : Filling graph\n",
      "2019-10-01 21:55:54,255 : INFO : Removing unreachable nodes of graph\n",
      "2019-10-01 21:55:54,258 : INFO : Pagerank graph\n",
      "2019-10-01 21:55:54,261 : INFO : Sorting pagerank scores\n",
      "2019-10-01 21:55:54,313 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-10-01 21:55:54,316 : INFO : built Dictionary(131 unique tokens: ['capabl', 'chain', 'constitut', 'respons', 'suppli']...) from 23 documents (total 294 corpus positions)\n",
      "2019-10-01 21:55:54,318 : INFO : Building graph\n",
      "2019-10-01 21:55:54,319 : INFO : Filling graph\n",
      "2019-10-01 21:55:54,326 : INFO : Removing unreachable nodes of graph\n",
      "2019-10-01 21:55:54,326 : INFO : Pagerank graph\n",
      "2019-10-01 21:55:54,332 : INFO : Sorting pagerank scores\n",
      "2019-10-01 21:55:54,371 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-10-01 21:55:54,372 : INFO : built Dictionary(77 unique tokens: ['barilla', 'consid', 'manufactur', 'pasta', 'custom']...) from 11 documents (total 143 corpus positions)\n",
      "2019-10-01 21:55:54,373 : INFO : Building graph\n",
      "2019-10-01 21:55:54,377 : INFO : Filling graph\n",
      "2019-10-01 21:55:54,379 : INFO : Removing unreachable nodes of graph\n",
      "2019-10-01 21:55:54,381 : INFO : Pagerank graph\n",
      "2019-10-01 21:55:54,385 : INFO : Sorting pagerank scores\n",
      "2019-10-01 21:55:54,423 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-10-01 21:55:54,424 : INFO : built Dictionary(83 unique tokens: ['countri', 'furnitur', 'ikea', 'larg', 'retail']...) from 18 documents (total 177 corpus positions)\n",
      "2019-10-01 21:55:54,426 : INFO : Building graph\n",
      "2019-10-01 21:55:54,427 : INFO : Filling graph\n",
      "2019-10-01 21:55:54,432 : INFO : Removing unreachable nodes of graph\n",
      "2019-10-01 21:55:54,434 : INFO : Pagerank graph\n",
      "2019-10-01 21:55:54,439 : INFO : Sorting pagerank scores\n",
      "2019-10-01 21:55:54,510 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-10-01 21:55:54,513 : INFO : built Dictionary(163 unique tokens: ['absorb', 'effici', 'impli', 'supplier', 'uncertainti']...) from 32 documents (total 460 corpus positions)\n",
      "2019-10-01 21:55:54,517 : INFO : Building graph\n",
      "2019-10-01 21:55:54,520 : INFO : Filling graph\n",
      "2019-10-01 21:55:54,529 : INFO : Removing unreachable nodes of graph\n",
      "2019-10-01 21:55:54,529 : INFO : Pagerank graph\n",
      "2019-10-01 21:55:54,536 : INFO : Sorting pagerank scores\n",
      "2019-10-01 21:55:54,592 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-10-01 21:55:54,593 : INFO : built Dictionary(83 unique tokens: ['chain', 'channel', 'compani', 'cost', 'custom']...) from 13 documents (total 163 corpus positions)\n",
      "2019-10-01 21:55:54,594 : INFO : Building graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-01 21:55:54,598 : INFO : Filling graph\n",
      "2019-10-01 21:55:54,601 : INFO : Removing unreachable nodes of graph\n",
      "2019-10-01 21:55:54,603 : INFO : Pagerank graph\n",
      "2019-10-01 21:55:54,606 : INFO : Sorting pagerank scores\n",
      "2019-10-01 21:55:54,645 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-10-01 21:55:54,646 : INFO : built Dictionary(69 unique tokens: ['chang', 'characterist', 'consid', 'cycl', 'demand']...) from 19 documents (total 134 corpus positions)\n",
      "2019-10-01 21:55:54,647 : INFO : Building graph\n",
      "2019-10-01 21:55:54,648 : INFO : Filling graph\n",
      "2019-10-01 21:55:54,652 : INFO : Removing unreachable nodes of graph\n",
      "2019-10-01 21:55:54,653 : INFO : Pagerank graph\n",
      "2019-10-01 21:55:54,659 : INFO : Sorting pagerank scores\n",
      "2019-10-01 21:55:54,694 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-10-01 21:55:54,695 : INFO : built Dictionary(88 unique tokens: ['build', 'capac', 'cycl', 'effici', 'flexibl']...) from 15 documents (total 191 corpus positions)\n",
      "2019-10-01 21:55:54,701 : INFO : Building graph\n",
      "2019-10-01 21:55:54,702 : INFO : Filling graph\n",
      "2019-10-01 21:55:54,707 : INFO : Removing unreachable nodes of graph\n",
      "2019-10-01 21:55:54,708 : INFO : Pagerank graph\n",
      "2019-10-01 21:55:54,717 : INFO : Sorting pagerank scores\n",
      "2019-10-01 21:55:54,742 : WARNING : Input text is expected to have at least 10 sentences.\n",
      "2019-10-01 21:55:54,743 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-10-01 21:55:54,744 : INFO : built Dictionary(49 unique tokens: ['competit', 'furnish', 'furnitur', 'home', 'low']...) from 8 documents (total 74 corpus positions)\n",
      "2019-10-01 21:55:54,746 : WARNING : Input corpus is expected to have at least 10 documents.\n",
      "2019-10-01 21:55:54,748 : INFO : Building graph\n",
      "2019-10-01 21:55:54,751 : INFO : Filling graph\n",
      "2019-10-01 21:55:54,753 : INFO : Removing unreachable nodes of graph\n",
      "2019-10-01 21:55:54,755 : INFO : Pagerank graph\n",
      "2019-10-01 21:55:54,759 : INFO : Sorting pagerank scores\n",
      "2019-10-01 21:55:54,851 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-10-01 21:55:54,853 : INFO : built Dictionary(264 unique tokens: ['chain', 'cost', 'devis', 'independ', 'intraoper']...) from 63 documents (total 728 corpus positions)\n",
      "2019-10-01 21:55:54,854 : INFO : Building graph\n",
      "2019-10-01 21:55:54,855 : INFO : Filling graph\n",
      "2019-10-01 21:55:54,873 : INFO : Removing unreachable nodes of graph\n",
      "2019-10-01 21:55:54,874 : INFO : Pagerank graph\n",
      "2019-10-01 21:55:54,887 : INFO : Sorting pagerank scores\n",
      "2019-10-01 21:55:54,965 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-10-01 21:55:54,966 : INFO : built Dictionary(64 unique tokens: ['euro', 'low', 'peak', 'valu', 'went']...) from 11 documents (total 94 corpus positions)\n",
      "2019-10-01 21:55:54,967 : INFO : Building graph\n",
      "2019-10-01 21:55:54,968 : INFO : Filling graph\n",
      "2019-10-01 21:55:54,969 : INFO : Removing unreachable nodes of graph\n",
      "2019-10-01 21:55:54,972 : INFO : Pagerank graph\n",
      "2019-10-01 21:55:54,976 : INFO : Sorting pagerank scores\n",
      "2019-10-01 21:55:55,045 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-10-01 21:55:55,047 : INFO : built Dictionary(246 unique tokens: ['chain', 'decad', 'firm', 'fragment', 'integr']...) from 46 documents (total 551 corpus positions)\n",
      "2019-10-01 21:55:55,049 : INFO : Building graph\n",
      "2019-10-01 21:55:55,050 : INFO : Filling graph\n",
      "2019-10-01 21:55:55,071 : INFO : Removing unreachable nodes of graph\n",
      "2019-10-01 21:55:55,072 : INFO : Pagerank graph\n",
      "2019-10-01 21:55:55,083 : INFO : Sorting pagerank scores\n"
     ]
    }
   ],
   "source": [
    "def Convert(string): \n",
    "    li = list(string.split(\"\\n\")) \n",
    "    return li \n",
    "    \n",
    "def target_summary_length(text, summary_length_ratio=0.2):\n",
    "    return int(len(text.split()) * summary_length_ratio)\n",
    "    \n",
    "for i in range(len(ParaSenList)):\n",
    "    ParaSen = ' '.join(ParaSenList[i])\n",
    "\n",
    "    centroid_word_embedding_summarizer = text_summarizer.CentroidWordEmbeddingsSummarizer(embedding_model, preprocess_type='nltk')\n",
    "    centroid_word_embedding_summary = centroid_word_embedding_summarizer.summarize(ParaSen)\n",
    "    \n",
    "    try:\n",
    "        gensim_summary = gensim.summarization.summarize(ParaSen)\n",
    "    except:\n",
    "        gensim_summary=\"input must have more than one sentence\"\n",
    "        \n",
    "\n",
    "    summa_summary = summa.summarizer.summarize(ParaSen, words=target_summary_length(ParaSen))\n",
    "        \n",
    "    summary_centroid=Convert(centroid_word_embedding_summary)\n",
    "    summary_gensim=Convert(gensim_summary)\n",
    "    summary_summa=Convert(summa_summary)\n",
    "\n",
    "\n",
    "    import csv\n",
    "    from itertools import zip_longest\n",
    "\n",
    "    d = [ParaSenList[i], summary_centroid, summary_gensim, summary_summa]\n",
    "    export_data = zip_longest(*d, fillvalue = '')\n",
    "    with open('./selectSentence/select_Sentence.csv', 'a', encoding='utf-8', newline='') as myfile:\n",
    "        wr = csv.writer(myfile)\n",
    "        wr.writerow((\"ParaSen\", \"Summary_centroid\", \"Summary_gensim\", \"Summary_summa\"))\n",
    "        wr.writerows(export_data)\n",
    "    myfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分開計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ParaSen = ' '.join(ParaSenList[1])\n",
    "centroid_word_embedding_summarizer = text_summarizer.CentroidWordEmbeddingsSummarizer(embedding_model, preprocess_type='nltk')\n",
    "centroid_word_embedding_summary = centroid_word_embedding_summarizer.summarize(ParaSen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It offers more than 500,000 products through both a catalog and a Web site.\n",
      "With this focus on responsiveness, McMaster does not compete based on low price.\n",
      "The value chain begins with new product development, which creates specifications for the product.\n",
      "It refers to consistency between the customer priorities that the competitive strategy hopes to satisfy and the supply chain capabilities that the supply chain strategy aims to build.\n"
     ]
    }
   ],
   "source": [
    "print(centroid_word_embedding_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#把字串轉list\n",
    "def Convert(string): \n",
    "    li = list(string.split(\"\\n\")) \n",
    "    return li \n",
    "summary=Convert(centroid_word_embedding_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分開寫檔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from itertools import zip_longest\n",
    "\n",
    "# d = [ParaSenList[4], summary]\n",
    "# export_data = zip_longest(*d, fillvalue = '')\n",
    "# with open('./selectSentence/select_Sentence.csv', 'a', encoding='utf-8', newline='') as myfile:\n",
    "#     wr = csv.writer(myfile)\n",
    "#     wr.writerow((\"ParaSen\", \"Summary\"))\n",
    "#     wr.writerows(export_data)\n",
    "# myfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0b2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
