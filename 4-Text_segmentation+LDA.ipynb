{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('./Chapter/Chapter-2.txt', 'r', encoding = 'utf-8-sig') as inFile:\n",
    "    Lines = inFile.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "Del = ['2.1','2.2','2.3','2.4','2.5','2.6']\n",
    "for lid in range(len(Lines)):\n",
    "    for d in Del:\n",
    "        if d in Lines[lid]:\n",
    "            Lines.remove(Lines) #要先打Lines[lid]執行過"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key Point\n",
      "\n",
      "Key Point\n",
      "\n",
      "Key Point\n",
      "\n",
      "Key Point\n",
      "\n",
      "Key Point\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sid in range(len(Lines)):\n",
    "    if len(Lines[sid])<=20 and Lines[sid] !='\\n' :\n",
    "        print(Lines[sid])\n",
    "        Lines[sid]=''\n",
    "while '' in Lines:\n",
    "    Lines.remove('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ''.join(Lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re;\n",
    "text=text.replace('\\n\\n\\n\\n', '\\n\\n')\n",
    "p=re.compile('\\n\\n',re.S)\n",
    "paraList=p.split(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "118"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(paraList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import TSF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg = -0.09032341682390929\n",
      "Dev = 1.2614359142391274\n"
     ]
    }
   ],
   "source": [
    "K = 2 # the number of sentences in a block\n",
    "THRESHOLD_DISSIM = 0.4 #根據Kern and Granitzer (2009)的建議訂為0.7。\n",
    "ParaSenList = TSF2.SegementPara(paraList, K, THRESHOLD_DISSIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 4\n",
      "1 13\n",
      "2 6\n",
      "3 2\n",
      "4 5\n",
      "5 2\n",
      "6 1\n",
      "7 8\n",
      "8 2\n",
      "9 14\n",
      "10 2\n",
      "11 2\n",
      "12 2\n",
      "13 2\n",
      "14 4\n",
      "15 2\n",
      "16 4\n",
      "17 2\n",
      "18 4\n",
      "19 2\n",
      "20 3\n",
      "21 1\n",
      "22 17\n",
      "23 2\n",
      "24 2\n",
      "25 2\n",
      "26 4\n",
      "27 4\n",
      "------\n",
      "28\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(ParaSenList)):\n",
    "    print (i,len(ParaSenList[i]))\n",
    "print('------')\n",
    "print(len(ParaSenList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(ParaSenList)):\n",
    "    if len(ParaSenList[i]) <=2 : #要執行三遍\n",
    "        ParaSenList[i-1].extend(ParaSenList[i])\n",
    "        try:\n",
    "            ParaSenList.remove(ParaSenList[i])\n",
    "        except ValueError:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 4\n",
      "1 13\n",
      "2 8\n",
      "3 8\n",
      "4 10\n",
      "5 16\n",
      "6 6\n",
      "7 6\n",
      "8 6\n",
      "9 6\n",
      "10 4\n",
      "11 19\n",
      "12 4\n",
      "13 4\n",
      "14 4\n",
      "------\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(ParaSenList)):\n",
    "    print (i,len(ParaSenList[i]))\n",
    "print('------')\n",
    "print(len(ParaSenList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "import numpy as np\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# create English stop words list\n",
    "en_stop = get_stop_words('en')\n",
    "\n",
    "# Create p_stemmer of class PorterStemmer\n",
    "p_stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/chenyifan/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/chenyifan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_set=[]\n",
    "texts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(len(ParaSenList)):\n",
    "    str1 = ' '.join(ParaSenList[k])\n",
    "    doc_set.append(str1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in doc_set:\n",
    "    \n",
    "    # clean and tokenize document string\n",
    "    raw = i.lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "\n",
    "    # remove stop words from tokens\n",
    "    stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "    \n",
    "    #仅保留名词或特定POS   \n",
    "    refiltered =nltk.pos_tag(stopped_tokens)\n",
    "    filtered = [w for w, pos in refiltered if pos.startswith('NN') or pos.startswith('NNP')]\n",
    "    \n",
    "    # stem tokens\n",
    "    lemma = nltk.wordnet.WordNetLemmatizer()    \n",
    "    stemmed_tokens = [lemma.lemmatize(i, pos='n') for i in filtered]\n",
    "    stemmed_tokens = [lemma.lemmatize(i, pos='v') for i in stemmed_tokens]\n",
    "\n",
    "    for k in range(len(stemmed_tokens)):\n",
    "        if len(stemmed_tokens[k])<3 :\n",
    "            stemmed_tokens[k]=''\n",
    "    while '' in stemmed_tokens:\n",
    "        stemmed_tokens.remove('')\n",
    "            \n",
    "    texts.append(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> I found that I had the best results with LDA when I additionally did some processing to remove the most common and the rarest words in the corpus\n",
    "- https://towardsdatascience.com/unsupervised-nlp-topic-models-as-a-supervised-learning-input-cf8ee9e5cf28\n",
    "- 用 *filter_extremes* 函式\n",
    "- http://blog.acring.me/2018/02/01/Gensim模块处理之dictionary和corpora/\n",
    "\n",
    "1. 去掉出现次数低于no_below的\n",
    "2. 去掉出现次数高于no_above的。注意这个小数指的是百分数\n",
    "3. 在1和2的基础上，保留出现频率前keep_n的单词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn our tokenized documents into a id <-> term dictionary\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "dictionary.filter_extremes(no_below=3, no_above=0.35)\n",
    "dictionary.compactify() \n",
    "#在执行完前面的过滤操作以后，可能会造成单词的序号之间有空隙，\n",
    "#这时就可以使用该函数来对词典来进行重新排序，去掉这些空隙。\n",
    "    \n",
    "# convert tokenized documents into a document-term matrix\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# generate LDA model\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=10, id2word = dictionary, passes=100)\n",
    "\n",
    "# # 列印前3個topic的詞分布\n",
    "# print(ldamodel.print_topics(num_topics=3, num_words=3))\n",
    "# # 列印id為0的topic的詞分布\n",
    "# print(ldamodel.print_topic(0, topn=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ==== 0.168*\"scope\" + 0.121*\"function\" + 0.088*\"inventory\" + 0.068*\"surplus\" + 0.054*\"operation\" + 0.041*\"transportation\" + 0.027*\"supplier\" + 0.027*\"action\" + 0.021*\"sale\" + 0.021*\"delivery\"\n",
      "1 ==== 0.121*\"environment\" + 0.061*\"technology\" + 0.061*\"opportunity\" + 0.061*\"issue\" + 0.061*\"market\" + 0.061*\"availability\" + 0.042*\"value\" + 0.022*\"factor\" + 0.022*\"lack\" + 0.022*\"challenge\"\n",
      "2 ==== 0.095*\"inventory\" + 0.082*\"supplier\" + 0.055*\"role\" + 0.055*\"capacity\" + 0.055*\"material\" + 0.055*\"response\" + 0.042*\"country\" + 0.042*\"production\" + 0.028*\"line\" + 0.028*\"efficient\"\n",
      "3 ==== 0.013*\"use\" + 0.013*\"technology\" + 0.013*\"line\" + 0.013*\"step\" + 0.013*\"package\" + 0.013*\"attribute\" + 0.013*\"volume\" + 0.013*\"value\" + 0.013*\"rate\" + 0.013*\"transportation\"\n",
      "4 ==== 0.013*\"opportunity\" + 0.013*\"mcmaster\" + 0.013*\"availability\" + 0.013*\"response\" + 0.013*\"quality\" + 0.013*\"purchase\" + 0.013*\"supplier\" + 0.013*\"place\" + 0.013*\"step\" + 0.013*\"operation\"\n",
      "5 ==== 0.074*\"segment\" + 0.074*\"place\" + 0.074*\"mcmaster\" + 0.056*\"availability\" + 0.056*\"convenience\" + 0.056*\"operation\" + 0.056*\"purchase\" + 0.038*\"quality\" + 0.038*\"importance\" + 0.038*\"response\"\n",
      "6 ==== 0.088*\"opportunity\" + 0.088*\"characteristic\" + 0.088*\"attribute\" + 0.088*\"plan\" + 0.088*\"satisfy\" + 0.008*\"quantity\" + 0.008*\"line\" + 0.008*\"step\" + 0.008*\"package\" + 0.008*\"volume\"\n",
      "7 ==== 0.154*\"carr\" + 0.104*\"step\" + 0.104*\"mcmaster\" + 0.053*\"choice\" + 0.053*\"inventory\" + 0.028*\"difficulty\" + 0.028*\"target\" + 0.028*\"discussion\" + 0.028*\"delivery\" + 0.028*\"capacity\"\n",
      "8 ==== 0.079*\"dell\" + 0.056*\"sale\" + 0.056*\"operation\" + 0.048*\"market\" + 0.048*\"distribution\" + 0.048*\"function\" + 0.040*\"transportation\" + 0.040*\"value\" + 0.040*\"inventory\" + 0.032*\"facility\"\n",
      "9 ==== 0.083*\"characteristic\" + 0.083*\"segment\" + 0.071*\"channel\" + 0.048*\"technology\" + 0.048*\"capability\" + 0.048*\"process\" + 0.036*\"rate\" + 0.025*\"difficulty\" + 0.025*\"industry\" + 0.025*\"quantity\"\n"
     ]
    }
   ],
   "source": [
    "# 列印所有topic\n",
    "for i in range(0, ldamodel.num_topics):\n",
    "    print(i,'====',ldamodel.print_topic(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(5, 0.9727247)]\n",
      "[(8, 0.99090785)]\n",
      "[(5, 0.94705456)]\n",
      "[(2, 0.957139)]\n",
      "[(0, 0.016666876), (1, 0.016667994), (2, 0.016667137), (3, 0.016666703), (4, 0.016666703), (5, 0.016668413), (6, 0.849994), (7, 0.016666703), (8, 0.016667211), (9, 0.016668238)]\n",
      "[(9, 0.9816309)]\n",
      "[(7, 0.9727255)]\n",
      "[(2, 0.9749977)]\n",
      "[(9, 0.9709644)]\n",
      "[(0, 0.011112074), (1, 0.89998823), (2, 0.011112005), (3, 0.011111165), (4, 0.011111165), (5, 0.011112746), (6, 0.011112977), (7, 0.011111164), (8, 0.011114385), (9, 0.011114133)]\n",
      "[(2, 0.93076295)]\n",
      "[(0, 0.99217284)]\n",
      "[(8, 0.96086395)]\n",
      "[(1, 0.974998)]\n",
      "[(0, 0.96896034)]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(doc_set)):\n",
    "    print(ldamodel.get_document_topics(corpus[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc 0 的主题： [5.] [0.97272468]\n",
      "doc 1 的主题： [8.] [0.99090785]\n",
      "doc 2 的主题： [5.] [0.94705445]\n",
      "doc 3 的主题： [2.] [0.95713902]\n",
      "doc 4 的主题： [6.] [0.84999406]\n",
      "doc 5 的主题： [9.] [0.98163098]\n",
      "doc 6 的主题： [7.] [0.97272563]\n",
      "doc 7 的主题： [2.] [0.9749977]\n",
      "doc 8 的主题： [9.] [0.97096455]\n",
      "doc 9 的主题： [1.] [0.89998811]\n",
      "doc 10 的主题： [2.] [0.93076295]\n",
      "doc 11 的主题： [0.] [0.99217284]\n",
      "doc 12 的主题： [8.] [0.96086395]\n",
      "doc 13 的主题： [1.] [0.974998]\n",
      "doc 14 的主题： [0.] [0.96895999]\n"
     ]
    }
   ],
   "source": [
    "M = len(texts)\n",
    "num_show_topic = 1  # 每个文档显示前几个主题\n",
    "doc_topics = ldamodel.get_document_topics(corpus)  # 所有文档的主题分布\n",
    "idx = np.arange(M)\n",
    "for i in idx:\n",
    "    topic = np.array(doc_topics[i])\n",
    "    topic_distribute = np.array(topic[:, 1])\n",
    "    # print topic_distribute\n",
    "    topic_idx = topic_distribute.argsort()[:-num_show_topic - 1:-1]\n",
    "    print(('doc %d 的主题：' % (i)), topic[topic_idx, 0], topic_distribute[topic_idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0b2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
